{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOuHEIVxeA6CTrRgQsLvYuT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":4,"metadata":{"id":"cJPxazy4iUzA","executionInfo":{"status":"ok","timestamp":1716119510861,"user_tz":-330,"elapsed":5,"user":{"displayName":"Himakar Sreerangam","userId":"06191167488964062518"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import tensorflow as tf\n","from sklearn import preprocessing\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.models import Sequential, Model\n","from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D, Input, Concatenate\n","from sklearn.model_selection import train_test_split\n","from keras.callbacks import EarlyStopping\n","from keras.layers import Dropout\n","from keras.utils import to_categorical\n","import re\n","import nltk\n","from nltk import RegexpTokenizer\n","from nltk.corpus import stopwords"]},{"cell_type":"code","source":["nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LrvQLpuOr9wu","executionInfo":{"status":"ok","timestamp":1716119620317,"user_tz":-330,"elapsed":661,"user":{"displayName":"Himakar Sreerangam","userId":"06191167488964062518"}},"outputId":"18a51767-4279-4ab9-dbc4-ab4ce8067bbb"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"code","source":["REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n","BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n","tokenizer = RegexpTokenizer(r'\\w+')\n","\n","def preprocess(text):\n","\n","  text = text.replace('{html}', \"\") # Remove weblinks\n","  text = text.lower()\n","  text = REPLACE_BY_SPACE_RE.sub(' ', text)\n","  text = BAD_SYMBOLS_RE.sub('', text)\n","  text = ' '.join(word for word in text.split() if word not in stop_words)\n","  tokens = tokenizer.tokenize(text)\n","  cleanedText = \" \".join(tokens)\n","\n","  return cleanedText"],"metadata":{"id":"Yhci28GIs0TY","executionInfo":{"status":"ok","timestamp":1716119637403,"user_tz":-330,"elapsed":4,"user":{"displayName":"Himakar Sreerangam","userId":"06191167488964062518"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv('complaints.csv')\n","df.dropna(subset = ['Consumer complaint narrative', 'Issue', 'sub-issue'], inplace = True)\n","df['input1'] = df['Consumer complaint narrative'].map(lambda x: preprocess(x))\n","df['input2'] = df['Issue']\n","df['input3'] = df['sub-issue']\n","df.head()"],"metadata":{"id":"Q61yaBlCs4gx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['product_id'] = df['Product'].factorize()[0]\n","product_id_df = df[['Product', 'product_id']].drop_duplicates().sort_values('product_id')\n","product_to_id = dict(product_id_df.values)\n","id_to_product = dict(product_id_df[['product_id', 'Product']].values)"],"metadata":{"id":"h5LrTRaYs-DG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = df[['input1', 'input2', 'input3']]\n","Y = df['Product']\n","\n","label_encoder = preprocessing.LabelEncoder()\n","\n","y = label_encoder.fit_transform(y)\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","y_train = to_categorical(y_train)\n","y_test = to_categorical(y_test)"],"metadata":{"id":"i5bgy2X9tzmL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MAX_NB_WORDS = 5000\n","MAX_SEQUENCE_LENGTH = 1000\n","EMBEDDING_DIM = 100\n","\n","\n","tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~', lower=True)\n","tokenizer.fit_on_texts(X['input1']+X['input2']+ X['input3'])\n","\n","X1_train = pad_sequences(tokenizer.text_to_sequences(X_train['input1']), maxlen=MAX_SEQUENCE_LENGTH)\n","X1_test = pad_sequences(tokenizer.text_to_sequences(X_test['input1']), maxlen=MAX_SEQUENCE_LENGTH)\n","word_index1 = tokenizer.word_index\n","vocab_size1 = len(word_index1) + 1\n","\n","X2_train = pad_sequences(tokenizer.text_to_sequences(X_train['input2']), maxlen=MAX_SEQUENCE_LENGTH)\n","X2_test = pad_sequences(tokenizer.text_to_sequences(X_test['input2']), maxlen=MAX_SEQUENCE_LENGTH)\n","word_index2 = tokenizer.word_index\n","vocab_size2 = len(word_index1) + 1\n","\n","X3_train = pad_sequences(tokenizer.text_to_sequences(X_train['input3']), maxlen=MAX_SEQUENCE_LENGTH)\n","X3_test = pad_sequences(tokenizer.text_to_sequences(X_test['input3']), maxlen=MAX_SEQUENCE_LENGTH)\n","word_index3 = tokenizer.word_index\n","vocab_size3 = len(word_index1) + 1\n","\n","input_1 = Input(shape=(MAX_SEQUENCE_LENGTH, ))\n","input_2 = Input(shape=(MAX_SEQUENCE_LENGTH, ))\n","input_3 = Input(shape=(MAX_SEQUENCE_LENGTH, ))"],"metadata":{"id":"t9bPn0LDvC6u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embedding_layer1 = Embedding(vocab_size1, 100, trainable=True)(input_1)\n","spatialdropout1 = SpatialDropout1D(0.2)(embedding_layer1)\n","LSTM_layer1 = LSTM(100, dropout=0.2, recurrent_dropout=0.2)(spatialdropout1)\n","\n","embedding_layer2 = Embedding(vocab_size1, 100, trainable=True)(input_2)\n","spatialdropout2 = SpatialDropout1D(0.2)(embedding_layer2)\n","LSTM_layer2 = LSTM(100, dropout=0.2, recurrent_dropout=0.2)(spatialdropout2)\n","\n","embedding_layer3 = Embedding(vocab_size1, 100, trainable=True)(input_3)\n","spatialdropout3 = SpatialDropout1D(0.2)(embedding_layer3)\n","LSTM_layer3 = LSTM(100, dropout=0.2, recurrent_dropout=0.2)(spatialdropout3)\n","\n","concat_layer = Concatenate()([LSTM_layer1, LSTM_layer2, LSTM_layer3])\n","dense_layer3 = Dense(50, activation='relu')(concat_layer)\n","dense_layer4 = Dense(10, activation='relu')(dense_layer3)\n","output= Dense(2, activation='softmax')(dense_layer4)\n","\n","model = Model(inputs=[input_1, input_2, input3], outputs = output)\n","\n","model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n","print(model.summary())"],"metadata":{"id":"JYLKHj7tz1ot"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["history = model.fit(x=[X1_train, X2_train, X3_train], y=y_train, batch_size = 32, epochs = 5, verbose = 1, validation_split = 0.1,\n","                    callbacks = [EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])"],"metadata":{"id":"_CqZinpA1RMj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["score = model.evaluate(x=[X1_test, X2_test, X3_test], y=y_test, verbose = 1)\n","\n","print(\"Test Score:\", score[0])\n","print(\"Test Accuracy:\", score[1])"],"metadata":{"id":"4oaNZCR81_v6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","\n","plt.title('Model Accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","plt.show()\n","\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","\n","plt.title('Model Loss')\n","plt.ylabel('Loss')\n","plt.xlabel('epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","plt.show()"],"metadata":{"id":"JiK4Vkq42O8x"},"execution_count":null,"outputs":[]}]}