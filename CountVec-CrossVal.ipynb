{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPBJdzTRP5fNJ9COnO5HYIp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"id":"Vk0dobMjYCwQ","executionInfo":{"status":"error","timestamp":1716114712577,"user_tz":-330,"elapsed":10,"user":{"displayName":"Himakar Sreerangam","userId":"06191167488964062518"}},"outputId":"4d7e36e2-dc9a-4113-d739-39bc037a988d"},"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-1-f3035fa09f71>, line 2)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-f3035fa09f71>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    import spacy_wordnet.wordnet_annotator import WordnetAnnotator\u001b[0m\n\u001b[0m                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}],"source":["import spacy\n","from spacy_wordnet.wordnet_annotator import WordnetAnnotator\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import SnowballStemmer # Stemming\n","from nltk.tokenize import RegexpTokenizer # Tokenizing\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_selection import chi2\n","import matplotlib.pyplot as plt\n","\n","from sklearn.naive_bayes import MultinomialNB\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import LinearSVC\n","from sklearn.model_selection import cross_val_score\n","import seaborn as sns"]},{"cell_type":"code","source":["df = pd.read_csv('complaints.csv')\n","df.head()"],"metadata":{"id":"nIIL6ukqaDQT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['product_id'] = df['Product'].factorize()[0]\n","product_id_df = df[['Product', 'product_id']].drop_duplicates().sort_values('product_id')\n","product_to_id = dict(product_id_df.values)\n","id_to_product = dict(product_id_df[['product_id', 'Product']].values)"],"metadata":{"id":"k2rFY4CpaYni"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = RegexpTokenizer(r'\\w+')\n","stemmer = SnowballStemmer(\"english\")\n","\n","def clean_text(text):\n","\n","  text = text.lower()\n","  text = text.replace('{html}', \"\")\n","  text = re.sub(r'[^\\w\\s]', ' ', text)\n","  text = re.sub('[0-9]+', '', text)\n","\n","  tokens = tokenizer(text)\n","  tokens = [stemmer.stem(t) for t in tokens]\n","  tokens = [t for t in tokens if len(t) > 2]\n","  cleanedText= \" \".join(tokens)\n","\n","  return cleanedText"],"metadata":{"id":"IPexmsWia7iU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['input'] = df['Consumer complaint narrative'].map(lambda x: clean_text(x))"],"metadata":{"id":"-FQmPzN8b3WB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))"],"metadata":{"id":"ZRY9eZZycnA_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["countVec = CountVectorizer(stop_words=stop_words)\n","features = countVec.fit_transform(df.input).toarray()\n","labels = df.product_id\n","features.shape"],"metadata":{"id":"YHFMSAhGck2J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["N=5\n","\n","for Product, product_id in sorted(product_to_id.items()):\n","\n","  features_chi2 = chi2(features, labels == product_id)\n","  indices = np.argsort(features_chi2[0])\n","  feature_names = np.array(countVec.get_feature_names())[indices]\n","\n","  unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n","  bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n","  trigrams = [v for v in feature_names if len(v.split(' ')) == 3]\n","\n","  print(\"# '{}':\".format(Product))\n","  print(\" . Most correalted unigrams:\\n. {}\".format('\\n. '.join(unigrams[-N:])))\n","  print(\" . Most correalted bigrams:\\n. {}\".format('\\n. '.join(bigrams[-N:])))\n","  print(\" . Most correalted trigrams:\\n. {}\".format('\\n. '.join(trigrams[-N:])))"],"metadata":{"id":"Z8FKBjm0djp7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Understanding Cross Validation:\n","###https://www.geeksforgeeks.org/cross-validation-machine-learning/"],"metadata":{"id":"lgg28AvphOV1"}},{"cell_type":"code","source":["models = [\n","    RandomForestClassifier(n_estimators = 200, max_depth = 3, random_state = 42)\n","    LinearSVC()\n","    MultinomialNB()\n","    LogisticRegression(random_state = 42)\n","]\n","\n","CV = 5\n","cv_df = pd.DataFrame(index=range(CV*len(models)))\n","entries = []\n","\n","for model in models:\n","\n","  model_name = model._class_._name_\n","  accuracies = cross_val_score(model, features, scoring = 'accuracy', cv=CV)\n","\n","  for fold_idx, accuracy in enumerate(accuracies):\n","\n","    entries.append((model_name, fold_idx, accuracy))\n","\n","cv_df = pd.DataFrame(entries, columns = ['model_name', 'fold_idx', 'accuracy'])\n","\n","sns.boxplot(x='model_name', y='accuracy', data=cv_df)\n","sns.stripplot(x='model_name', y='accuracy', data=cv_df, size=8, jitter=True, edgecolour='grey', linewidth=2)\n","\n","plt.show()"],"metadata":{"id":"ohoNbDSbesgM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cv_df.groupby('model_name').accuracy.mean()"],"metadata":{"id":"Iata7Sl3g1QH"},"execution_count":null,"outputs":[]}]}