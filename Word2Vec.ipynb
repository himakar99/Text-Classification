{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOtpgeMskh/fet1Od2BqO7j"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":378},"id":"t-Zo_5VdSt8q","executionInfo":{"status":"error","timestamp":1716113643145,"user_tz":-330,"elapsed":10,"user":{"displayName":"Himakar Sreerangam","userId":"06191167488964062518"}},"outputId":"b2bdaee4-71d9-4827-f9ee-06e23388f71f"},"outputs":[{"output_type":"error","ename":"ImportError","evalue":"cannot import name 'LinearSVM' from 'sklearn.svm' (/usr/local/lib/python3.10/dist-packages/sklearn/svm/__init__.py)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-9436950add1e>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnaive_bayes\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msvm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearSVM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'LinearSVM' from 'sklearn.svm' (/usr/local/lib/python3.10/dist-packages/sklearn/svm/__init__.py)","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import pandas as pd\n","import numpy as np\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from sklearn.model_selection import train_test_split\n","from nltk.tokenize import RegexpTokenizer\n","import string\n","from gensim.models import Word2Vec\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.svm import LinearSVC\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"]},{"cell_type":"code","source":["nltk.download('stopwords')\n","stop_words = set(stopwords.words('english'))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1XRKHO8sTYiD","executionInfo":{"status":"ok","timestamp":1716112995201,"user_tz":-330,"elapsed":904,"user":{"displayName":"Himakar Sreerangam","userId":"06191167488964062518"}},"outputId":"4cfbbc1e-4dde-4d71-f102-53a12dbd223b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}]},{"cell_type":"code","source":["REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n","BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n","tokenizer = RegexpTokenizer(r'\\w+')\n","\n","def preprocess(text):\n","\n","  text = text.replace('{html}', \"\") # Remove weblinks\n","  text = text.lower()\n","  text = REPLACE_BY_SPACE_RE.sub(' ', text)\n","  text = BAD_SYMBOLS_RE.sub('', text)\n","  text = ' '.join(word for word in text.split() if word not in stop_words)\n","  tokens = tokenizer.tokenize(text)\n","  cleanedText = \" \".join(tokens)\n","\n","  return cleanedText"],"metadata":{"id":"uOpo4AiDTcef"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv('complaints.csv')\n","df.head()"],"metadata":{"id":"fT-saGvKToCa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.dropna(subset = ['Consumer complaint narrative'], inplace = True)\n","df['input'] = df['Consumer complaint narrative'].map(lambda x: preprocess(x))"],"metadata":{"id":"HjkaleTKTxGt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Understanding Word2Vec:\n","###https://www.geeksforgeeks.org/python-word-embedding-using-word2vec/"],"metadata":{"id":"CqjnTsbqVGaC"}},{"cell_type":"code","source":["X_train, y_train, X_test, y_yest = train_test_split(df['input'], df['Product'], test_size = 0.2, random_state = 42)\n","\n","sentences =  [sentence.split() for sentence in X_train]\n","w2v_model = Word2Vec(sentences,\n","                     size=500,\n","                     window=5,\n","                     min_count=5\n","                     )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":105},"id":"YG4LaWLtUK_D","executionInfo":{"status":"error","timestamp":1716113462941,"user_tz":-330,"elapsed":21,"user":{"displayName":"Himakar Sreerangam","userId":"06191167488964062518"}},"outputId":"cdcf2507-bd92-4af0-aff4-a6f8a2d85b4f"},"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"invalid syntax (<ipython-input-6-a8c806e94c07>, line 3)","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-a8c806e94c07>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    sentences [sentence.split() for sentence in X_train]\u001b[0m\n\u001b[0m                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"code","source":["def vectorize(sentence):\n","\n","  words = sentence.split()\n","  words_vecs = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n","  if len(words_vec) == 0:\n","\n","    return np.zeros(100)\n","\n","  words_vecs = np.array(words_vecs)\n","\n","  return words_vecs.mean(axis=0)"],"metadata":{"id":"fga8iR69UrVD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train = np.array([vectorize(sentence) for sentence in X_train])\n","X_test = np.array([vectorize(sentence) for sentence in X_test])"],"metadata":{"id":"HyY-hEndVZbg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["logReg = LogisticRegression()\n","logReg.fit(X_train, y_train)\n","y_pred = logReg.predict(X_test)\n","print('Logistic Regression Accuracy:', accuracy_score(y_test, y_pred))\n","\n","RF = RandomForestClassifier(n_estimators=200, max_depth=3, random_state=42)\n","RF.fit(X_train, y_train)\n","y_pred = RF.predict(X_test)\n","print('Random Forest Accuracy:', accuracy_score(y_test, y_pred))\n","\n","LSVC = LinearSVC()\n","LSVC.fit(X_train, y_train)\n","y_pred = LSVC.predict(X_test)\n","print('SVM Accuracy:', accuracy_score(y_test, y_pred))"],"metadata":{"id":"1MFxcN2eVlt3"},"execution_count":null,"outputs":[]}]}